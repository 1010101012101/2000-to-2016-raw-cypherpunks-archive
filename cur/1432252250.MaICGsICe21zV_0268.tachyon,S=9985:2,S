From rswatjfet.org+caf_=rsw=jfet.org@gmail.com  Mon Nov  3 07:42:18 2014
Return-Path: <rswatjfet.org+caf_=rsw=jfet.org@gmail.com>
Received: from mail-pa0-f51.google.com (mail-pa0-f51.google.com [209.85.220.51])
	by antiproton.jfet.org (8.14.4/8.14.4/Debian-4.1) with ESMTP id sA3CgHMj029435
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=NOT)
	for <rsw@jfet.org>; Mon, 3 Nov 2014 07:42:18 -0500
Received: by mail-pa0-f51.google.com with SMTP id kq14so12161606pab.10
        for <rsw@jfet.org>; Mon, 03 Nov 2014 04:41:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-original-authentication-results:delivered-to:date:to:subject
         :message-id:mime-version:content-type:content-disposition
         :content-transfer-encoding:user-agent:precedence:list-id
         :list-unsubscribe:list-archive:list-post:list-help:list-subscribe
         :errors-to:sender:from;
        bh=eQ84AdKEwOFxTMIpYH7dw84xg5XsfMfHLgyan+geuY8=;
        b=M5DSw+6jwAUtTF/eX5Rg4SBd5GgIZETgpc5v1kMM+ISd2m1KOqmF81/bTKBjEbbfNw
         qYVuFnP0uVKyltXmSVEecrkwAgMO/I3tKuhEe93Bhl7YV59nfBMUzMfDTF8XcPhlfOuR
         2KN2hOngxTFUVkiZw588UbJQH5zMc/rss3zQ7zyZmV3jVW73c0g82sqwjN2oAFQ0gM6a
         ZI2v5JEi54hE0qMqakfVmY9PUg3bqu41uwG0roo9bJ3T79W06sxEynoyHd7rawl68iCV
         fxIhoPuxwVMwhrguQmi3TMNHeL7gFcnwDiDD90U9d1vMSFkNYvA2pYJEOJD9pmFLWCs7
         pknA==
X-Original-Authentication-Results: mx.google.com;       spf=neutral (google.com: 209.141.47.85 is neither permitted nor denied by domain of rsw+cypherpunks-bounces=cpunks.org@gloop.phonon.net) smtp.mail=rsw+cypherpunks-bounces=cpunks.org@gloop.phonon.net
X-Received: by 10.68.130.134 with SMTP id oe6mr42503478pbb.3.1415018515671;
        Mon, 03 Nov 2014 04:41:55 -0800 (PST)
X-Forwarded-To: rsw@jfet.org
X-Forwarded-For: rswatjfet.org@gmail.com rsw@jfet.org
Delivered-To: rswatjfet.org@gmail.com
Received: by 10.70.135.1 with SMTP id po1csp260704pdb;
        Mon, 3 Nov 2014 04:41:54 -0800 (PST)
X-Received: by 10.70.100.199 with SMTP id fa7mr3661799pdb.114.1415018514676;
        Mon, 03 Nov 2014 04:41:54 -0800 (PST)
Received: from localhost (antiproton.jfet.org. [209.141.47.85])
        by mx.google.com with ESMTPS id fk8si15325790pab.13.2014.11.03.04.41.52
        for <rswATjfet.org@gmail.com>
        (version=TLSv1.2 cipher=RC4-SHA bits=128/128);
        Mon, 03 Nov 2014 04:41:52 -0800 (PST)
Received-SPF: neutral (google.com: 209.141.47.85 is neither permitted nor denied by domain of rsw+cypherpunks-bounces=cpunks.org@gloop.phonon.net) client-ip=209.141.47.85;
Authentication-Results: mx.google.com;
       spf=neutral (google.com: 209.141.47.85 is neither permitted nor denied by domain of rsw+cypherpunks-bounces=cpunks.org@gloop.phonon.net) smtp.mail=rsw+cypherpunks-bounces=cpunks.org@gloop.phonon.net
Received: from antiproton.jfet.org (localhost.localdomain [127.0.0.1])
	by antiproton.jfet.org (8.14.4/8.14.4/Debian-4.1) with ESMTP id sA3Cejkq029415;
	Mon, 3 Nov 2014 07:40:52 -0500
Received: from leitl.org (v8.ativel.com [164.177.174.8])
 by antiproton.jfet.org (8.14.4/8.14.4/Debian-4.1) with ESMTP id sA3CeZck029411
 (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=NOT)
 for <cypherpunks@cpunks.org>; Mon, 3 Nov 2014 07:40:37 -0500
Received: by leitl.org (Postfix, from userid 1000)
 id 13AB05410B3; Mon,  3 Nov 2014 13:40:16 +0100 (CET)
Date: Mon, 3 Nov 2014 13:40:15 +0100
To: tt@postbiota.org, neuro@postbiota.org, cypherpunks@cpunks.org
Subject: Brain decoder can eavesdrop on your inner voice
Message-ID: <20141103124015.GC10467@leitl.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
User-Agent: Mutt/1.5.21 (2010-09-15)
X-BeenThere: cypherpunks@cpunks.org
X-Mailman-Version: 2.1.18
Precedence: list
List-Id: The Cypherpunks Mailing List <cypherpunks.cpunks.org>
List-Unsubscribe: <https://cpunks.org/mailman/options/cypherpunks>,
 <mailto:cypherpunks-request@cpunks.org?subject=unsubscribe>
List-Archive: <http://cpunks.org/pipermail/cypherpunks/>
List-Post: <mailto:cypherpunks@cpunks.org>
List-Help: <mailto:cypherpunks-request@cpunks.org?subject=help>
List-Subscribe: <https://cpunks.org/mailman/listinfo/cypherpunks>,
 <mailto:cypherpunks-request@cpunks.org?subject=subscribe>
Errors-To: cypherpunks-bounces@cpunks.org
Sender: "cypherpunks" <cypherpunks-bounces@cpunks.org>
From: Eugen Leitl <eugen@leitl.org>
X-Gspam-Loop: antiproton.jfet.org
Status: O
Content-Length: 5455
Lines: 105


http://www.newscientist.com/article/mg22429934.000-brain-decoder-can-eavesdrop-on-your-inner-voice.html

Brain decoder can eavesdrop on your inner voice

29 October 2014 by Helen Thomson

Magazine issue 2993. Subscribe and save

For similar stories, visit the The Human Brain Topic Guide

As you read this, your neurons are firing – that brain activity can now
be decoded to reveal the silent words in your head

TALKING to yourself used to be a strictly private pastime. That's no longer
the case – researchers have eavesdropped on our internal monologue for
the first time. The achievement is a step towards helping people who cannot
physically speak communicate with the outside world.

"If you're reading text in a newspaper or a book, you hear a voice in your
own head," says Brian Pasley at the University of California, Berkeley.
"We're trying to decode the brain activity related to that voice to create a
medical prosthesis that can allow someone who is paralysed or locked in to
speak."

When you hear someone speak, sound waves activate sensory neurons in your
inner ear. These neurons pass information to areas of the brain where
different aspects of the sound are extracted and interpreted as words.

In a previous study, Pasley and his colleagues recorded brain activity in
people who already had electrodes implanted in their brain to treat epilepsy,
while they listened to speech. The team found that certain neurons in the
brain's temporal lobe were only active in response to certain aspects of
sound, such as a specific frequency. One set of neurons might only react to
sound waves that had a frequency of 1000 hertz, for example, while another
set only cares about those at 2000 hertz. Armed with this knowledge, the team
built an algorithm that could decode the words heard based on neural activity
aloneMovie Camera (PLoS Biology, doi.org/fzv269).

The team hypothesised that hearing speech and thinking to oneself might spark
some of the same neural signatures in the brain. They supposed that an
algorithm trained to identify speech heard out loud might also be able to
identify words that are thought.

Mind-reading

To test the idea, they recorded brain activity in another seven people
undergoing epilepsy surgery, while they looked at a screen that displayed
text from either the Gettysburg Address, John F. Kennedy's inaugural address
or the nursery rhyme Humpty Dumpty.

Each participant was asked to read the text aloud, read it silently in their
head and then do nothing. While they read the text out loud, the team worked
out which neurons were reacting to what aspects of speech and generated a
personalised decoder to interpret this information. The decoder was used to
create a spectrogram – a visual representation of the different
frequencies of sound waves heard over time. As each frequency correlates to
specific sounds in each word spoken, the spectrogram can be used to recreate
what had been said. They then applied the decoder to the brain activity that
occurred while the participants read the passages silently to themselves (see
diagram).

Despite the neural activity from imagined or actual speech differing
slightly, the decoder was able to reconstruct which words several of the
volunteers were thinking, using neural activity alone (Frontiers in
Neuroengineering, doi.org/whb).

The algorithm isn't perfect, says Stephanie Martin, who worked on the study
with Pasley. "We got significant results but it's not good enough yet to
build a device."

In practice, if the decoder is to be used by people who are unable to speak
it would have to be trained on what they hear rather than their own speech.
"We don't think it would be an issue to train the decoder on heard speech
because they share overlapping brain areas," says Martin.

The team is now fine-tuning their algorithms, by looking at the neural
activity associated with speaking rate and different pronunciations of the
same word, for example. "The bar is very high," says Pasley. "Its preliminary
data, and we're still working on making it better."

The team have also turned their hand to predicting what songs a person is
listening to by playing lots of Pink Floyd to volunteers, and then working
out which neurons respond to what aspects of the music. "Sound is sound,"
says Pasley. "It all helps us understand different aspects of how the brain
processes it."

"Ultimately, if we understand covert speech well enough, we'll be able to
create a medical prosthesis that could help someone who is paralysed, or
locked in and can't speak," he says.

Several other researchers are also investigating ways to read the human mind.
Some can tell what pictures a person is looking at, others have worked out
what neural activity represents certain concepts in the brain, and one team
has even produced crude reproductions of movie clips that someone is watching
just by analysing their brain activity. So is it possible to put it all
together to create one multisensory mind-reading device?

In theory, yes, says Martin, but it would be extraordinarily complicated. She
says you would need a huge amount of data for each thing you are trying to
predict. "It would be really interesting to look into. It would allow us to
predict what people are doing or thinking," she says. "But we need individual
decoders that work really well before combining different senses."

This article appeared in print under the headline "Hearing our inner voice"

