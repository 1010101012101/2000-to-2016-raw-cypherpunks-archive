From owner-cypherpunks@al-qaeda.net  Mon Jul  4 12:52:06 2011
Return-Path: <owner-cypherpunks@al-qaeda.net>
Received: from proton.jfet.org (localhost.localdomain [127.0.0.1])
	by proton.jfet.org (8.14.3/8.14.3/Debian-9.4) with ESMTP id p64GmW19009605
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT)
	for <cypherpunks-outgoing@proton.jfet.org>; Mon, 4 Jul 2011 12:48:32 -0400
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=al-qaeda.net;
	s=cpunks; t=1309798112;
	bh=uWBddz1TIxoxTmY0MHo9Zb6isu+WAX4kJZfOc7uGJ0o=;
	h=Date:From:To:Subject:Message-ID:MIME-Version:Content-Type:Sender;
	b=NBVd8tqQMQIZxe7MUzs+Zc69eQ52/4n/4PIrEVBNeN4ltm1ACuz0YbA9DI4f5SniT
	 s7qUpJvCba405LBwrnmLbFNLeooXDmQEuuosL9PdY8IsNaDsFm5T23iSOJRlBoWMCy
	 PkmnQoGLwb131taF5NuDDFG9oZDHoCgcg7AWEgoE=
Received: (from majordomo@localhost)
	by proton.jfet.org (8.14.3/8.14.3/Submit) id p64GmW5Y009604
	for cypherpunks-outgoing; Mon, 4 Jul 2011 12:48:32 -0400
Date: Mon, 4 Jul 2011 18:48:29 +0200
From: Eugen Leitl <eugen@leitl.org>
To: tt@postbiota.org, cypherpunks@al-qaeda.net
Subject: Kinect knows what you are doing... 
Message-ID: <20110704164829.GI28500@leitl.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
User-Agent: Mutt/1.5.18 (2008-05-17)
Sender: owner-cypherpunks@al-qaeda.net
Precedence: bulk
X-Loop: al-qaeda.net
Status: O
Content-Length: 3110
Lines: 76

http://www.i-programmer.info/news/105-artificial-intelligence/2695-kinect-knows-what-you-are-doing.html

Kinect knows what you are doing...

Written by Mike James   

Monday, 04 July 2011 08:30

You can train a Kinect to recognize what you are doing and perhaps even to
recognize who is doing it!

Just when you thought that the Kinect RGBD (Red, Green, Blue, Depth) camera
had gone stale and that there is nothing left to do with it,  researchers
from Cornell prove you wrong.

What they have done is to take a standard Kinect with the open source drivers
and the PrimeSense Nite software and created a program that can tell what you
are doing. The Kinect mounted on a roaming robot or perhaps one in every room
can monitor what you are doing - cleaning your teeth, cooking, writing on a
whiteboard (the designer are academics after all) and so on.

Why would you want to do this?

Imagine the intelligent house of the future. It is obvious that it knowing
what you were doing would be an advantage -

"Would you like some help with that recipe, Dave?"

At a less ambitious level you could use a Kinect to monitor patients say and
make sure that they were drinking or eating etc. A more worrying application
might be to make sure that workers were doing just that - working and doing
the correct task.

The researchers have also demonstrated that it isn't only Microsoft Research
who can put AI into Kinect. They used a hierarchical maximum entropy Markov
model which is based on identifying sub-activities such as "pickup", "drink",
"place" and so on.

 

kinectactivityFour samples from the training site - brushing teeth, cooking,
relaxing on chair, opening pill container.

 

The training set was small compared to the sort of thing that Microsoft
Research uses and hence the results aren't as convincing. Only four different
people were used and simply given an instruction to perform the activity but
in full view of the Kinect. This is a bit artificial in that you don't
usually clean your teeth standing in front of a Kinect. The results of the
training worked well for subjects seen before - 84.31% correct activity.

What is equally interesting is that the classification rate dropped to 64.17%
for people that hadn't been seen before. What this means is that there might
well be information contained in how people do these standard tasks
sufficient to recognize the individuals. It might well be that how you clean
your teeth is sufficient to identify you.

To turn either possible application into a reality would require a much
bigger training set and further development - but it looks promising.

It also demonstrates that it is not just Microsoft Research that can add AI
methods to the Kinect. Most of its creative uses to this point have simply
been adaptations that make use of its depth input to guide robots or gesture
recognition to control quadrotors etc. In this case the unique view of the
world that a depth camera provides has been used within machine learning to
produce something completely new.

 

kinect1

 
More information:

Human Activity Detection from RGBD Images (Pdf)

